import cv2
import numpy as np
from ultralytics import YOLO
import supervision as sv
from deep_sort_realtime.deepsort_tracker import DeepSort
from collections import defaultdict, deque
from coordinates import get_coordinates

# –ò–º–ø–æ—Ä—Ç —Ñ—É–Ω–∫—Ü–∏–π –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏
from speed_by_shift import compute_speed_shift
from speed_by_lines import update_vehicle_timestamp, compute_speed_line

def is_above_line(cx, cy, x1, y1, x2, y2):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ª–∏ —Ç–æ—á–∫–∞ (cx, cy) –≤—ã—à–µ –ª–∏–Ω–∏–∏, –∑–∞–¥–∞–Ω–Ω–æ–π –¥–≤—É–º—è —Ç–æ—á–∫–∞–º–∏ (x1, y1) –∏ (x2, y2).
    """
    if x1 == x2:
        return cx <= x1
    m = (y2 - y1) / (x2 - x1)
    b = y1 - m * x1
    y_on_line = m * cx + b
    return cy <= y_on_line

def main():
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ YOLO
    model = YOLO("yolo11x.pt")

    # –ó–∞–≥—Ä—É–∑–∫–∞ –≤–∏–¥–µ–æ
    video_path = "50kmh_prieksa_jaunolaine.mov"
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print("–û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –≤–∏–¥–µ–æ.")
        return

    # –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –∏–∑ coordinates.py
    try:
        (x1, y1, x2, y2, x3, y3, x4, y4, distance_m,
         blue_x1_top, blue_y1_top, blue_x2_top, blue_y2_top,
         blue_x1_bottom, blue_y1_bottom, blue_x2_bottom, blue_y2_bottom,
         blue_line_thickness) = get_coordinates(video_path)
    except ValueError as e:
        print(e)
        cap.release()
        return

    # –ß—Ç–µ–Ω–∏–µ –ø–µ—Ä–≤–æ–≥–æ –∫–∞–¥—Ä–∞
    ret, frame = cap.read()
    if not ret:
        print("–û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å —Å—á–∏—Ç–∞—Ç—å –∫–∞–¥—Ä.")
        cap.release()
        return

    # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤–∏–¥–µ–æ
    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = int(cap.get(cv2.CAP_PROP_FPS))

    # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
    screen_width, screen_height = 1280, 720
    scale = min(screen_width / frame_width, screen_height / frame_height)
    new_width = int(frame_width * scale)
    new_height = int(frame_height * scale)

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∑–∞–ø–∏—Å–∏ –≤–∏–¥–µ–æ
    output_path = "output.mp4"
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))

    # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –∫ –º–∞—Å—à—Ç–∞–±—É
    x1, x2, x3, x4 = int(x1 * scale), int(x2 * scale), int(x3 * scale), int(x4 * scale)
    y1, y2, y3, y4 = int(y1 * scale), int(y2 * scale), int(y3 * scale), int(y4 * scale)
    blue_x1_top, blue_x2_top = int(blue_x1_top * scale), int(blue_x2_top * scale)
    blue_y1_top, blue_y2_top = int(blue_y1_top * scale), int(blue_y2_top * scale)
    blue_x1_bottom, blue_x2_bottom = int(blue_x1_bottom * scale), int(blue_x2_bottom * scale)
    blue_y1_bottom, blue_y2_bottom = int(blue_y1_bottom * scale), int(blue_y2_bottom * scale)

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–∞—Ç—Ä–∏—Ü—ã –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è
    src = np.array([
        [blue_x1_top, blue_y1_top],
        [blue_x2_top, blue_y2_top],
        [blue_x1_bottom, blue_y1_bottom],
        [blue_x2_bottom, blue_y2_bottom]
    ], dtype=np.float32)

    dst = np.array([
        [0, 0],
        [6, 0],
        [0, distance_m],
        [6, distance_m]
    ], dtype=np.float32)

    matrix = cv2.getPerspectiveTransform(src, dst)

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–±–ª–∞—Å—Ç–∏ –∏–Ω—Ç–µ—Ä–µ—Å–∞ (–∫—Ä–∞—Å–Ω–∞—è —Ä–∞–º–∫–∞)
    pts = np.array([[x1, y1], [x2, y2], [x3, y3], [x4, y4]], np.int32)

    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–µ—Ä–≤–æ–≥–æ –∫–∞–¥—Ä–∞ —Å —Ä–∞–º–∫–æ–π –∏ –ª–∏–Ω–∏—è–º–∏
    frame_with_zone = frame.copy()
    cv2.polylines(frame_with_zone, [pts], isClosed=True, color=(0, 0, 255), thickness=3)
    cv2.line(frame_with_zone, (blue_x1_top, blue_y1_top), (blue_x2_top, blue_y2_top), (255, 0, 0), blue_line_thickness)
    cv2.line(frame_with_zone, (blue_x1_bottom, blue_y1_bottom), (blue_x2_bottom, blue_y2_bottom), (255, 0, 0), blue_line_thickness)
    resized_frame = cv2.resize(frame_with_zone, (new_width, new_height))
    cv2.imshow("–û–±–ª–∞—Å—Ç—å –∏–Ω—Ç–µ—Ä–µ—Å–∞ (–ø–µ—Ä–≤—ã–π –∫–∞–¥—Ä)", resized_frame)
    cv2.waitKey(0)
    cv2.destroyAllWindows()

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–Ω–æ—Ç–∞—Ç–æ—Ä–æ–≤
    box_annotator = sv.BoxAnnotator(color=sv.Color.GREEN, thickness=2)
    label_annotator = sv.LabelAnnotator(text_color=sv.Color.BLACK)

    # –ò—Å—Ç–æ—Ä–∏—è –¥–ª—è –º–µ—Ç–æ–¥–∞ —Å–¥–≤–∏–≥–∞ (—Ö—Ä–∞–Ω–∏–º –∏—Å—Ç–æ—Ä–∏—é —Ä–µ–∞–ª—å–Ω—ã—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç)
    real_y_history = defaultdict(lambda: deque(maxlen=int(fps)))

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–∫–µ—Ä–∞ Deep SORT
    tracker = DeepSort(max_age=30)

    # –°–ª–æ–≤–∞—Ä—å –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ (–º–µ—Ç–æ–¥ –ª–∏–Ω–∏–π)
    vehicle_timestamps = {}

    # –•—Ä–∞–Ω–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–µ–π –ø–æ –º–µ—Ç–æ–¥—É —Å–¥–≤–∏–≥–∞
    vehicle_speeds_shift = defaultdict(list)

    frame_count = 0
    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä—Ö–Ω–µ–π –∏ –Ω–∏–∂–Ω–µ–π –ª–∏–Ω–∏–π –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏ (–∫–æ—Ä—Ç–µ–∂–∏: (x1, y1, x2, y2))
    top_line = (blue_x1_top, blue_y1_top, blue_x2_top, blue_y2_top)
    bottom_line = (blue_x1_bottom, blue_y1_bottom, blue_x2_bottom, blue_y2_bottom)

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        frame_time = frame_count / fps
        frame_count += 1

        results = model(frame)
        boxes_for_tracking = []

        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–µ—Ç–µ–∫—Ü–∏–π –¥–ª—è —Ç—Ä–µ–∫–µ—Ä–∞
        for result in results:
            for box in result.boxes:
                x_min, y_min, x_max, y_max = map(int, box.xyxy[0])
                conf = box.conf[0].item()
                cls = int(box.cls[0].item())

                # –û—Ç—Å–µ–∏–≤–∞–µ–º –≤—Å—ë, –∫—Ä–æ–º–µ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π (–∫–ª–∞—Å—Å 2)
                if cls != 2:
                    continue

                if not cv2.pointPolygonTest(pts, ((x_min + x_max) // 2, (y_min + y_max) // 2), False) >= 0:
                    continue

                boxes_for_tracking.append(([x_min, y_min, x_max - x_min, y_max - y_min], conf, 'car'))

        tracks = tracker.update_tracks(boxes_for_tracking, frame=frame)
        
        tracked_boxes = []
        confidences = []
        class_ids = []
        labels = []

        for track in tracks:
            if not track.is_confirmed():
                continue

            track_id = track.track_id
            l, t, r, b = map(int, track.to_ltrb())
            cx = (l + r) // 2
            cy = b

                    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –≤ —Ä–µ–∞–ª—å–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ
            real_point = cv2.perspectiveTransform(
                np.array([[[cx, cy]]], dtype=np.float32),
                matrix
            )[0][0]

            real_y_history[track_id].append(real_point)

            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ –¥–ª—è –º–µ—Ç–æ–¥–∞ –ª–∏–Ω–∏–π
            if track_id not in vehicle_timestamps:
                vehicle_timestamps[track_id] = {"start": None, "end": None, "last_position": cy, "done": False}
            if not vehicle_timestamps[track_id].get("done"):
                update_vehicle_timestamp(vehicle_timestamps[track_id], cx, cy, frame_time, top_line, bottom_line, is_above_line)

            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ –ø–æ –º–µ—Ç–æ–¥—É —Å–¥–≤–∏–≥–∞ —Ç–æ–ª—å–∫–æ –º–µ–∂–¥—É —Å–∏–Ω–∏–º–∏ –ª–∏–Ω–∏—è–º–∏
            speed_transformed = None
            if is_above_line(cx, cy, blue_x1_bottom, blue_y1_bottom, blue_x2_bottom, blue_y2_bottom) and \
            not is_above_line(cx, cy, blue_x1_top, blue_y1_top, blue_x2_top, blue_y2_top):
                last_speed = vehicle_speeds_shift[track_id][-1] if vehicle_speeds_shift[track_id] else None
                speed_transformed = compute_speed_shift(real_y_history[track_id], fps, last_speed)

            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–¥–ø–∏—Å–∏ —Å –æ–±–µ–∏–º–∏ —Å–∫–æ—Ä–æ—Å—Ç—è–º–∏
            label_parts = []
            if speed_transformed is not None:
                label_parts.append(f"T: {speed_transformed:.1f} km/h")
            speed_line = compute_speed_line(vehicle_timestamps[track_id], distance_m)
            if speed_line is not None:
                label_parts.append(f"L: {speed_line:.1f} km/h")
            if not label_parts:
                label_parts.append("Tracking...")

            labels.append(" | ".join(label_parts))
            tracked_boxes.append([l, t, r, b])
            confidences.append(1.0)
            class_ids.append(2)

        if tracked_boxes:
            detections_sv = sv.Detections(
                xyxy=np.array(tracked_boxes),
                confidence=np.array(confidences),
                class_id=np.array(class_ids)
            )
            frame = box_annotator.annotate(scene=frame, detections=detections_sv)
            frame = label_annotator.annotate(scene=frame, detections=detections_sv, labels=labels)

        cv2.line(frame, (blue_x1_top, blue_y1_top), (blue_x2_top, blue_y2_top), (255, 0, 0), blue_line_thickness)
        cv2.line(frame, (blue_x1_bottom, blue_y1_bottom), (blue_x2_bottom, blue_y2_bottom), (255, 0, 0), blue_line_thickness)
        cv2.polylines(frame, [pts], isClosed=True, color=(0, 0, 255), thickness=2)

        out.write(frame)

    # –í—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤—ã—Ö —Å–∫–æ—Ä–æ—Å—Ç–µ–π (–º–µ—Ç–æ–¥ –ª–∏–Ω–∏–π)
    print("\nüìä –ò—Ç–æ–≥–æ–≤—ã–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ (–ª–∏–Ω–∏—è):")
    for vehicle_id, times in vehicle_timestamps.items():
        speed_kmh = compute_speed_line(times, distance_m)
        if speed_kmh is not None:
            print(f"üöó –ú–∞—à–∏–Ω–∞ {vehicle_id}: L: {speed_kmh:.2f} –∫–º/—á")

    # –í—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤—ã—Ö —Å–∫–æ—Ä–æ—Å—Ç–µ–π (–º–µ—Ç–æ–¥ —Å–¥–≤–∏–≥–∞)
    print("\nüìä –ò—Ç–æ–≥–æ–≤—ã–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ (—Å–¥–≤–∏–≥):")
    for vehicle_id, speeds in vehicle_speeds_shift.items():
        if speeds:
            avg_speed_shift = sum(speeds) / len(speeds)
            print(f"üöó –ú–∞—à–∏–Ω–∞ {vehicle_id}: T: {avg_speed_shift:.2f} –∫–º/—á")

    cap.release()
    out.release()
    cv2.destroyAllWindows()
    print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ –≤–∏–¥–µ–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∫–∞–∫ {output_path}")

if __name__ == "__main__":
    main()
